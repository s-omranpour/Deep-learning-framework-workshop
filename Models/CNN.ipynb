{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "modules, losses, activations, etc. :\n",
    " - https://pytorch.org/docs/stable/nn.html\n",
    " \n",
    "optimizers and schedulers:\n",
    " - https://pytorch.org/docs/stable/optim.html\n",
    " \n",
    "examples:\n",
    " - https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    " - https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, in_channel, out_channels, kernels, strides, dropouts):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(out_channels)):\n",
    "            in_c = in_channel if i ==0 else out_channels[i-1]\n",
    "            out_c = out_channels[i]\n",
    "            k = kernels[i]\n",
    "            s = strides[i]\n",
    "            d = dropouts[i]\n",
    "            input_size = input_size // s\n",
    "            layers += [nn.Conv2d(in_c, out_c, k, s, padding=(k-s+1)//2), \n",
    "                       nn.BatchNorm2d(out_c),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Dropout(d)]\n",
    "        layers += [nn.Flatten(), nn.Linear(out_c*out_c*input_size, num_classes), nn.BatchNorm2d(num_classes), nn.Softmax()]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Conv2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Flatten()\n",
      "    (13): Linear(in_features=7168, out_features=10, bias=True)\n",
      "    (14): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cls = Classifier(28, 10, 1, [8,16,32], [5,5,3], [2,2,1], [0.2,0.2,0.2])\n",
    "print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(self):\n",
    "    params = []\n",
    "    for param in self.parameters():\n",
    "        a = 1\n",
    "        for s in param.shape:\n",
    "            a *= s\n",
    "        params += [a]\n",
    "    return sum(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, path, name):\n",
    "    torch.save(self.state_dict(), path + str(name) + '.pkl')\n",
    "\n",
    "def load(model, path, name):\n",
    "    self.load_state_dict(torch.load(path + str(name) + '.pkl'))\n",
    "\n",
    "def train(model, t_loader, v_loader, criterion, optimizer, epochs=100, verbose=10, chechpoint=200):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        # TRAINING LOOP\n",
    "        i = 0\n",
    "        for train_batch in t_loader:\n",
    "            x, y = train_batch\n",
    "\n",
    "            logits = model(x.cuda())\n",
    "            loss = criterion(logits, y.cuda())\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc = accuracy_score(torch.argmax(y,dim=1).numpy(), torch.argmax(logits,dim=1).cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print('\\r',i,end='')\n",
    "            i+=1\n",
    "\n",
    "        train_loss = np.mean(train_loss)\n",
    "        train_acc = np.mean(train_acc)\n",
    "        log = '\\r{} train_loss:{} train_acc:{}\\n'.format(epoch+1, train_loss, train_acc)\n",
    "        log += evaluate(model, v_loader, criterion)\n",
    "\n",
    "        if (epoch+1) % verbose == 0:\n",
    "            print(log)\n",
    "\n",
    "        if (epoch+1) % chechpoint == 0:\n",
    "            save(model,'weights/', 'cls_'+str(epoch+1))\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        for val_batch in loader:\n",
    "            x, y = val_batch\n",
    "            logits = model(x.cuda())\n",
    "            val_loss.append(criterion(logits, y.cuda()).item())\n",
    "            val_acc = accuracy_score(torch.argmax(y,dim=1).numpy(), torch.argmax(logits,dim=1).cpu().numpy())\n",
    "\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_acc = np.mean(val_acc)\n",
    "        return 'val_loss:{} val_acc:{}\\n'.format(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(cls.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
